{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "883af077",
   "metadata": {},
   "source": [
    "# Hate Speech Detection With Python\n",
    "https://copyassignment.com/hate-speech-detection/\n",
    "\n",
    "根据这个教程的方法，使用另一个更大的数据集来训练数据，看看能不能有更好的效果\n",
    "\n",
    "数据集：https://huggingface.co/datasets/ucberkeley-dlab/measuring-hate-speech\n",
    "\n",
    "数据来源：https://hatespeech.berkeley.edu/\n",
    "\n",
    "This is a public release of the dataset described in Kennedy et al. (2020) and Sachdeva et al. (2022), consisting of 39,565 comments annotated by 7,912 annotators, for 135,556 combined rows. The primary outcome variable is the **\"hate speech score\" but the 10 constituent ordinal labels (sentiment, (dis)respect, insult, humiliation, inferior status, violence, dehumanization, genocide, attack/defense, hate speech benchmark)** can also be treated as outcomes. Includes 8 target identity groups (race/ethnicity, religion, national origin/citizenship, gender, sexual orientation, age, disability, political ideology) and 42 target identity subgroups, as well as 6 annotator demographics and 40 subgroups. The hate speech score incorporates an IRT adjustment by estimating variation in annotator interpretation of the labeling guidelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5c19278",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:04:18.397233Z",
     "start_time": "2023-02-27T11:04:18.365764Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn. feature_extraction. text import CountVectorizer\n",
    "from sklearn. feature_extraction. text import TfidfVectorizer\n",
    "from sklearn. model_selection import train_test_split\n",
    "from sklearn. tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcf322be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:04:20.888194Z",
     "start_time": "2023-02-27T11:04:19.845863Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "#nltk. download('stopwords')\n",
    "from nltk. corpus import stopwords\n",
    "stopword=set(stopwords.words('english'))\n",
    "stemmer = nltk. SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b47a53ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:04:29.018595Z",
     "start_time": "2023-02-27T11:04:22.692447Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>annotator_id</th>\n",
       "      <th>platform</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>respect</th>\n",
       "      <th>insult</th>\n",
       "      <th>humiliate</th>\n",
       "      <th>status</th>\n",
       "      <th>dehumanize</th>\n",
       "      <th>violence</th>\n",
       "      <th>...</th>\n",
       "      <th>annotator_religion_hindu</th>\n",
       "      <th>annotator_religion_jewish</th>\n",
       "      <th>annotator_religion_mormon</th>\n",
       "      <th>annotator_religion_muslim</th>\n",
       "      <th>annotator_religion_nothing</th>\n",
       "      <th>annotator_religion_other</th>\n",
       "      <th>annotator_sexuality_bisexual</th>\n",
       "      <th>annotator_sexuality_gay</th>\n",
       "      <th>annotator_sexuality_straight</th>\n",
       "      <th>annotator_sexuality_other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47777</td>\n",
       "      <td>10873</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39773</td>\n",
       "      <td>2790</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47101</td>\n",
       "      <td>3379</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43625</td>\n",
       "      <td>7365</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12538</td>\n",
       "      <td>488</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   comment_id  annotator_id  platform  sentiment  respect  insult  humiliate  \\\n",
       "0       47777         10873         3        0.0      0.0     0.0        0.0   \n",
       "1       39773          2790         2        0.0      0.0     0.0        0.0   \n",
       "2       47101          3379         3        4.0      4.0     4.0        4.0   \n",
       "3       43625          7365         3        2.0      3.0     2.0        1.0   \n",
       "4       12538           488         0        4.0      4.0     4.0        4.0   \n",
       "\n",
       "   status  dehumanize  violence  ...  annotator_religion_hindu  \\\n",
       "0     2.0         0.0       0.0  ...                     False   \n",
       "1     2.0         0.0       0.0  ...                     False   \n",
       "2     4.0         4.0       0.0  ...                     False   \n",
       "3     2.0         0.0       0.0  ...                     False   \n",
       "4     4.0         4.0       4.0  ...                     False   \n",
       "\n",
       "   annotator_religion_jewish  annotator_religion_mormon  \\\n",
       "0                      False                      False   \n",
       "1                      False                      False   \n",
       "2                      False                      False   \n",
       "3                      False                      False   \n",
       "4                      False                      False   \n",
       "\n",
       "   annotator_religion_muslim annotator_religion_nothing  \\\n",
       "0                      False                      False   \n",
       "1                      False                      False   \n",
       "2                      False                       True   \n",
       "3                      False                      False   \n",
       "4                      False                      False   \n",
       "\n",
       "   annotator_religion_other  annotator_sexuality_bisexual  \\\n",
       "0                     False                         False   \n",
       "1                     False                         False   \n",
       "2                     False                         False   \n",
       "3                     False                         False   \n",
       "4                     False                         False   \n",
       "\n",
       "   annotator_sexuality_gay  annotator_sexuality_straight  \\\n",
       "0                    False                          True   \n",
       "1                    False                          True   \n",
       "2                    False                          True   \n",
       "3                    False                          True   \n",
       "4                    False                          True   \n",
       "\n",
       "   annotator_sexuality_other  \n",
       "0                      False  \n",
       "1                      False  \n",
       "2                      False  \n",
       "3                      False  \n",
       "4                      False  \n",
       "\n",
       "[5 rows x 131 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd. read_csv('measuring_hate_speech.csv')\n",
    "#To preview the data\n",
    "data. head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743633d3",
   "metadata": {},
   "source": [
    "## Key dataset columns\n",
    "hate_speech_score - continuous hate speech measure, where higher = more hateful and lower = less hateful. > 0.5 is approximately hate speech, < -1 is counter or supportive speech, and -1 to +0.5 is neutral or ambiguous.\n",
    "text - lightly processed text of a social media post\n",
    "- comment_id - unique ID for each comment\n",
    "- annotator_id - unique ID for each annotator\n",
    "- sentiment - ordinal label that is combined into the continuous score\n",
    "- respect - ordinal label that is combined into the continuous score\n",
    "- insult - ordinal label that is combined into the continuous score\n",
    "- humiliate - ordinal label that is combined into the continuous score\n",
    "- status - ordinal label that is combined into the continuous score\n",
    "- dehumanize - ordinal label that is combined into the continuous score\n",
    "- violence - ordinal label that is combined into the continuous score\n",
    "- genocide - ordinal label that is combined into the continuous score\n",
    "- attack_defend - ordinal label that is combined into the continuous score\n",
    "- hatespeech - ordinal label that is combined into the continuous score\n",
    "- annotator_severity - annotator's estimated survey interpretation bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5589deec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:04:40.929481Z",
     "start_time": "2023-02-27T11:04:40.569484Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>annotator_id</th>\n",
       "      <th>platform</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>respect</th>\n",
       "      <th>insult</th>\n",
       "      <th>humiliate</th>\n",
       "      <th>status</th>\n",
       "      <th>dehumanize</th>\n",
       "      <th>violence</th>\n",
       "      <th>...</th>\n",
       "      <th>hatespeech</th>\n",
       "      <th>hate_speech_score</th>\n",
       "      <th>infitms</th>\n",
       "      <th>outfitms</th>\n",
       "      <th>annotator_severity</th>\n",
       "      <th>std_err</th>\n",
       "      <th>annotator_infitms</th>\n",
       "      <th>annotator_outfitms</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>annotator_age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.00000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135556.000000</td>\n",
       "      <td>135451.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>23530.416138</td>\n",
       "      <td>5567.097812</td>\n",
       "      <td>1.281352</td>\n",
       "      <td>2.954307</td>\n",
       "      <td>2.828875</td>\n",
       "      <td>2.56331</td>\n",
       "      <td>2.278638</td>\n",
       "      <td>2.698575</td>\n",
       "      <td>1.846211</td>\n",
       "      <td>1.052045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.744733</td>\n",
       "      <td>-0.567428</td>\n",
       "      <td>1.034322</td>\n",
       "      <td>1.001052</td>\n",
       "      <td>-0.018817</td>\n",
       "      <td>0.300588</td>\n",
       "      <td>1.007158</td>\n",
       "      <td>1.011841</td>\n",
       "      <td>0.014589</td>\n",
       "      <td>37.910772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12387.194125</td>\n",
       "      <td>3230.508937</td>\n",
       "      <td>1.023542</td>\n",
       "      <td>1.231552</td>\n",
       "      <td>1.309548</td>\n",
       "      <td>1.38983</td>\n",
       "      <td>1.370876</td>\n",
       "      <td>0.898500</td>\n",
       "      <td>1.402372</td>\n",
       "      <td>1.345706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.932260</td>\n",
       "      <td>2.380003</td>\n",
       "      <td>0.496867</td>\n",
       "      <td>0.791943</td>\n",
       "      <td>0.487261</td>\n",
       "      <td>0.236380</td>\n",
       "      <td>0.269876</td>\n",
       "      <td>0.675863</td>\n",
       "      <td>0.613006</td>\n",
       "      <td>11.641276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-8.340000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>-1.820000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>-1.578693</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>18148.000000</td>\n",
       "      <td>2719.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.330000</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>-0.380000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>-0.341008</td>\n",
       "      <td>29.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>20052.000000</td>\n",
       "      <td>5602.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.340000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>-0.020000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.110405</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>32038.250000</td>\n",
       "      <td>8363.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.410000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>1.220000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>1.170000</td>\n",
       "      <td>1.130000</td>\n",
       "      <td>0.449555</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>50070.000000</td>\n",
       "      <td>11142.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>5.900000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.360000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>2.010000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.987511</td>\n",
       "      <td>81.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          comment_id   annotator_id       platform      sentiment  \\\n",
       "count  135556.000000  135556.000000  135556.000000  135556.000000   \n",
       "mean    23530.416138    5567.097812       1.281352       2.954307   \n",
       "std     12387.194125    3230.508937       1.023542       1.231552   \n",
       "min         1.000000       1.000000       0.000000       0.000000   \n",
       "25%     18148.000000    2719.000000       0.000000       2.000000   \n",
       "50%     20052.000000    5602.500000       1.000000       3.000000   \n",
       "75%     32038.250000    8363.000000       2.000000       4.000000   \n",
       "max     50070.000000   11142.000000       3.000000       4.000000   \n",
       "\n",
       "             respect        insult      humiliate         status  \\\n",
       "count  135556.000000  135556.00000  135556.000000  135556.000000   \n",
       "mean        2.828875       2.56331       2.278638       2.698575   \n",
       "std         1.309548       1.38983       1.370876       0.898500   \n",
       "min         0.000000       0.00000       0.000000       0.000000   \n",
       "25%         2.000000       2.00000       1.000000       2.000000   \n",
       "50%         3.000000       3.00000       3.000000       3.000000   \n",
       "75%         4.000000       4.00000       3.000000       3.000000   \n",
       "max         4.000000       4.00000       4.000000       4.000000   \n",
       "\n",
       "          dehumanize       violence  ...     hatespeech  hate_speech_score  \\\n",
       "count  135556.000000  135556.000000  ...  135556.000000      135556.000000   \n",
       "mean        1.846211       1.052045  ...       0.744733          -0.567428   \n",
       "std         1.402372       1.345706  ...       0.932260           2.380003   \n",
       "min         0.000000       0.000000  ...       0.000000          -8.340000   \n",
       "25%         1.000000       0.000000  ...       0.000000          -2.330000   \n",
       "50%         2.000000       0.000000  ...       0.000000          -0.340000   \n",
       "75%         3.000000       2.000000  ...       2.000000           1.410000   \n",
       "max         4.000000       4.000000  ...       2.000000           6.300000   \n",
       "\n",
       "             infitms       outfitms  annotator_severity        std_err  \\\n",
       "count  135556.000000  135556.000000       135556.000000  135556.000000   \n",
       "mean        1.034322       1.001052           -0.018817       0.300588   \n",
       "std         0.496867       0.791943            0.487261       0.236380   \n",
       "min         0.100000       0.070000           -1.820000       0.020000   \n",
       "25%         0.710000       0.560000           -0.380000       0.030000   \n",
       "50%         0.960000       0.830000           -0.020000       0.340000   \n",
       "75%         1.300000       1.220000            0.350000       0.420000   \n",
       "max         5.900000       9.000000            1.360000       1.900000   \n",
       "\n",
       "       annotator_infitms  annotator_outfitms     hypothesis  annotator_age  \n",
       "count      135556.000000       135556.000000  135556.000000  135451.000000  \n",
       "mean            1.007158            1.011841       0.014589      37.910772  \n",
       "std             0.269876            0.675863       0.613006      11.641276  \n",
       "min             0.390000            0.280000      -1.578693      18.000000  \n",
       "25%             0.810000            0.670000      -0.341008      29.000000  \n",
       "50%             0.970000            0.850000       0.110405      35.000000  \n",
       "75%             1.170000            1.130000       0.449555      45.000000  \n",
       "max             2.010000            9.000000       0.987511      81.000000  \n",
       "\n",
       "[8 rows x 22 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81422384",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:06:59.869745Z",
     "start_time": "2023-02-27T11:06:59.830979Z"
    }
   },
   "outputs": [],
   "source": [
    "#filter the dataset\n",
    "subdata = data[['text','hate_speech_score','hatespeech']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adc7b157",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:07:01.185546Z",
     "start_time": "2023-02-27T11:07:01.167966Z"
    }
   },
   "outputs": [],
   "source": [
    "# > 0.5 is approximately hate speech, < -1 is counter or supportive speech, and -1 to +0.5 is neutral or ambiguous.\n",
    "def score2hate(score):\n",
    "    if score>0.5:\n",
    "        return 1\n",
    "    if score <-1:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97276cf8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:07:01.727237Z",
     "start_time": "2023-02-27T11:07:01.680313Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>hate_speech_score</th>\n",
       "      <th>hatespeech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes indeed. She sort of reminds me of the elde...</td>\n",
       "      <td>-3.90</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The trans women reading this tweet right now i...</td>\n",
       "      <td>-6.52</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Question: These 4 broads who criticize America...</td>\n",
       "      <td>0.36</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It is about time for all illegals to go back t...</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>For starters bend over the one in pink and kic...</td>\n",
       "      <td>1.54</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135551</th>\n",
       "      <td>عاجل سماحة #السيد_عبدالملك_بدرالدين_الحوثي  نص...</td>\n",
       "      <td>-4.88</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135552</th>\n",
       "      <td>Millions of #Yemen-is participated in mass ral...</td>\n",
       "      <td>-4.40</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135553</th>\n",
       "      <td>@AbeShinzo @realDonaldTrump @shinzoabe 独裁者は行きま...</td>\n",
       "      <td>-2.49</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135554</th>\n",
       "      <td>Millions of #Yemen-is participated in mass ral...</td>\n",
       "      <td>-4.40</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135555</th>\n",
       "      <td>لا تتشمت الرجال مسكين يعاني كس امه 😂. يقول يال...</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135556 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  hate_speech_score  \\\n",
       "0       Yes indeed. She sort of reminds me of the elde...              -3.90   \n",
       "1       The trans women reading this tweet right now i...              -6.52   \n",
       "2       Question: These 4 broads who criticize America...               0.36   \n",
       "3       It is about time for all illegals to go back t...               0.26   \n",
       "4       For starters bend over the one in pink and kic...               1.54   \n",
       "...                                                   ...                ...   \n",
       "135551  عاجل سماحة #السيد_عبدالملك_بدرالدين_الحوثي  نص...              -4.88   \n",
       "135552  Millions of #Yemen-is participated in mass ral...              -4.40   \n",
       "135553  @AbeShinzo @realDonaldTrump @shinzoabe 独裁者は行きま...              -2.49   \n",
       "135554  Millions of #Yemen-is participated in mass ral...              -4.40   \n",
       "135555  لا تتشمت الرجال مسكين يعاني كس امه 😂. يقول يال...              -0.20   \n",
       "\n",
       "        hatespeech  \n",
       "0              0.0  \n",
       "1              0.0  \n",
       "2              2.0  \n",
       "3              0.0  \n",
       "4              2.0  \n",
       "...            ...  \n",
       "135551         0.0  \n",
       "135552         0.0  \n",
       "135553         0.0  \n",
       "135554         0.0  \n",
       "135555         2.0  \n",
       "\n",
       "[135556 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb93047a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:07:03.695201Z",
     "start_time": "2023-02-27T11:07:03.571949Z"
    }
   },
   "outputs": [],
   "source": [
    "subdata['is_hate'] = subdata['hate_speech_score'].apply(score2hate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1876a1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:07:05.034373Z",
     "start_time": "2023-02-27T11:07:05.012943Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>hate_speech_score</th>\n",
       "      <th>hatespeech</th>\n",
       "      <th>is_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes indeed. She sort of reminds me of the elde...</td>\n",
       "      <td>-3.90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The trans women reading this tweet right now i...</td>\n",
       "      <td>-6.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Question: These 4 broads who criticize America...</td>\n",
       "      <td>0.36</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It is about time for all illegals to go back t...</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>For starters bend over the one in pink and kic...</td>\n",
       "      <td>1.54</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135551</th>\n",
       "      <td>عاجل سماحة #السيد_عبدالملك_بدرالدين_الحوثي  نص...</td>\n",
       "      <td>-4.88</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135552</th>\n",
       "      <td>Millions of #Yemen-is participated in mass ral...</td>\n",
       "      <td>-4.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135553</th>\n",
       "      <td>@AbeShinzo @realDonaldTrump @shinzoabe 独裁者は行きま...</td>\n",
       "      <td>-2.49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135554</th>\n",
       "      <td>Millions of #Yemen-is participated in mass ral...</td>\n",
       "      <td>-4.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135555</th>\n",
       "      <td>لا تتشمت الرجال مسكين يعاني كس امه 😂. يقول يال...</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135556 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  hate_speech_score  \\\n",
       "0       Yes indeed. She sort of reminds me of the elde...              -3.90   \n",
       "1       The trans women reading this tweet right now i...              -6.52   \n",
       "2       Question: These 4 broads who criticize America...               0.36   \n",
       "3       It is about time for all illegals to go back t...               0.26   \n",
       "4       For starters bend over the one in pink and kic...               1.54   \n",
       "...                                                   ...                ...   \n",
       "135551  عاجل سماحة #السيد_عبدالملك_بدرالدين_الحوثي  نص...              -4.88   \n",
       "135552  Millions of #Yemen-is participated in mass ral...              -4.40   \n",
       "135553  @AbeShinzo @realDonaldTrump @shinzoabe 独裁者は行きま...              -2.49   \n",
       "135554  Millions of #Yemen-is participated in mass ral...              -4.40   \n",
       "135555  لا تتشمت الرجال مسكين يعاني كس امه 😂. يقول يال...              -0.20   \n",
       "\n",
       "        hatespeech  is_hate  \n",
       "0              0.0       -1  \n",
       "1              0.0       -1  \n",
       "2              2.0        0  \n",
       "3              0.0        0  \n",
       "4              2.0        1  \n",
       "...            ...      ...  \n",
       "135551         0.0       -1  \n",
       "135552         0.0       -1  \n",
       "135553         0.0       -1  \n",
       "135554         0.0       -1  \n",
       "135555         2.0        0  \n",
       "\n",
       "[135556 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0019ae7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:07:10.421363Z",
     "start_time": "2023-02-27T11:07:10.401691Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fuck off you insufferable retarded faggot.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdata[subdata['is_hate']==1].text.iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2769cc8c",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "In Data preprocessing, we prepare the raw data and make it suitable for a machine learning model. It is the first and crucial step while creating a machine learning model. When creating a machine learning project, it is not always a case that we come across clean and formatted data. And while doing any operation with data, it is mandatory to clean it and put it in a formatted way. So for this, we use the data preprocessing task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9d16cdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:07:14.417666Z",
     "start_time": "2023-02-27T11:07:14.360702Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes indeed. She sort of reminds me of the elde...</td>\n",
       "      <td>supportive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The trans women reading this tweet right now i...</td>\n",
       "      <td>supportive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Question: These 4 broads who criticize America...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It is about time for all illegals to go back t...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>For starters bend over the one in pink and kic...</td>\n",
       "      <td>hate speech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text        label\n",
       "0  Yes indeed. She sort of reminds me of the elde...   supportive\n",
       "1  The trans women reading this tweet right now i...   supportive\n",
       "2  Question: These 4 broads who criticize America...      neutral\n",
       "3  It is about time for all illegals to go back t...      neutral\n",
       "4  For starters bend over the one in pink and kic...  hate speech"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdata[\"label\"] = subdata[\"is_hate\"]. map({0: \"neutral\", 1: \"hate speech\", -1: \"supportive\"})\n",
    "subdata = subdata[[\"text\", \"label\"]]\n",
    "subdata. head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "14a88b4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:18:08.541238Z",
     "start_time": "2023-02-27T11:18:08.458705Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>starter bend one pink kick ass pussi get tast ...</td>\n",
       "      <td>hate speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fuck insuffer retard faggot</td>\n",
       "      <td>hate speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>white small littl invis clit disgrac 🤢 pay sma...</td>\n",
       "      <td>hate speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>suddenlywestan countri mental ill terrorist wh...</td>\n",
       "      <td>hate speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sub full grade faggot  comedi fuck ebola</td>\n",
       "      <td>hate speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122428</th>\n",
       "      <td>stop suck fatass dick niggerlov kike</td>\n",
       "      <td>hate speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122429</th>\n",
       "      <td>stupid asian bitch your idiot go back ching ch...</td>\n",
       "      <td>hate speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122430</th>\n",
       "      <td>stupid asian bitch your idiot go back ching ch...</td>\n",
       "      <td>hate speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122431</th>\n",
       "      <td>fuck fagot burn hell</td>\n",
       "      <td>hate speech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122434</th>\n",
       "      <td>nigger liber friend steal everyth tie like pre...</td>\n",
       "      <td>hate speech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49048 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text        label\n",
       "4       starter bend one pink kick ass pussi get tast ...  hate speech\n",
       "7                             fuck insuffer retard faggot  hate speech\n",
       "14      white small littl invis clit disgrac 🤢 pay sma...  hate speech\n",
       "17      suddenlywestan countri mental ill terrorist wh...  hate speech\n",
       "18               sub full grade faggot  comedi fuck ebola  hate speech\n",
       "...                                                   ...          ...\n",
       "122428               stop suck fatass dick niggerlov kike  hate speech\n",
       "122429  stupid asian bitch your idiot go back ching ch...  hate speech\n",
       "122430  stupid asian bitch your idiot go back ching ch...  hate speech\n",
       "122431                               fuck fagot burn hell  hate speech\n",
       "122434  nigger liber friend steal everyth tie like pre...  hate speech\n",
       "\n",
       "[49048 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdata[subdata.label =='hate speech']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f827ca4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:08:32.994617Z",
     "start_time": "2023-02-27T11:07:22.814277Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Windows\\Temp\\ipykernel_14520\\2463516109.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subdata[\"text\"] = subdata[\"text\"]. apply(clean)\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "def clean (text):\n",
    "    text = str (text). lower()\n",
    "    text = re. sub('[.?]', '', text) \n",
    "    text = re. sub('https?://\\S+|www.\\S+', '', text)\n",
    "    text = re. sub('<.?>+', '', text)\n",
    "    text = re. sub('[%s]' % re. escape(string. punctuation), '', text)\n",
    "    text = re. sub('\\n', '', text)\n",
    "    text = re. sub('\\w\\d\\w', '', text)\n",
    "    text = [word for word in text.split(' ') if word not in stopword]\n",
    "    text=\" \". join(text)\n",
    "    text = [stemmer. stem(word) for word in text. split(' ')]\n",
    "    text=\" \". join(text)\n",
    "    return text\n",
    "\n",
    "subdata[\"text\"] = subdata[\"text\"]. apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99aa7a7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:08:37.975060Z",
     "start_time": "2023-02-27T11:08:37.925873Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "supportive     53651\n",
       "hate speech    49048\n",
       "neutral        32857\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdata.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb2dcdc",
   "metadata": {},
   "source": [
    "## Splitting the data\n",
    "The next important step is to explore the dataset and divide the dataset into training and testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eeab79",
   "metadata": {},
   "source": [
    "NLP三种词袋模型CountVectorizer/TFIDF/HashVectorizer\n",
    "https://zhuanlan.zhihu.com/p/268886634"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f4f270b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:08:45.105606Z",
     "start_time": "2023-02-27T11:08:45.082829Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np. array(subdata[\"text\"])\n",
    "y = np. array(subdata[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4ed5134",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:08:51.134109Z",
     "start_time": "2023-02-27T11:08:47.062227Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "X = cv. fit_transform(x)\n",
    "# Splitting the Data\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "#StratifiedKFold 分层k折\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "skf.get_n_splits(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c9ce28a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:08:52.227165Z",
     "start_time": "2023-02-27T11:08:51.774167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 11128  11129  11130 ... 135553 135554 135555] TEST: [    0     1     2 ... 18499 18501 18503]\n",
      "TRAIN: [     0      1      2 ... 135553 135554 135555] TEST: [11128 11129 11130 ... 37097 37105 37106]\n",
      "TRAIN: [     0      1      2 ... 135553 135554 135555] TEST: [22146 22154 22159 ... 55224 55226 55228]\n",
      "TRAIN: [     0      1      2 ... 135553 135554 135555] TEST: [33103 33106 33110 ... 73695 73698 73701]\n",
      "TRAIN: [     0      1      2 ... 135553 135554 135555] TEST: [44339 44342 44344 ... 92005 92008 92010]\n",
      "TRAIN: [     0      1      2 ... 135553 135554 135555] TEST: [55315 55320 55326 ... 97512 97513 97514]\n",
      "TRAIN: [     0      1      2 ... 135553 135554 135555] TEST: [ 66269  66274  66277 ... 102909 102910 102911]\n",
      "TRAIN: [     0      1      2 ... 135553 135554 135555] TEST: [ 77389  77390  77398 ... 124346 124347 124349]\n",
      "TRAIN: [     0      1      2 ... 135553 135554 135555] TEST: [ 88408  88409  88411 ... 130186 130187 130188]\n",
      "TRAIN: [     0      1      2 ... 130186 130187 130188] TEST: [113774 113775 113776 ... 135553 135554 135555]\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in skf.split(X, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "489618f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:08:58.669536Z",
     "start_time": "2023-02-27T11:08:53.213505Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 11128  11129  11130 ... 135553 135554 135555] TEST: [    0     1     2 ... 18499 18501 18503]\n",
      "TRAIN: [     0      1      2 ... 135553 135554 135555] TEST: [11128 11129 11130 ... 37097 37105 37106]\n",
      "TRAIN: [     0      1      2 ... 135553 135554 135555] TEST: [22146 22154 22159 ... 55224 55226 55228]\n",
      "TRAIN: [     0      1      2 ... 135553 135554 135555] TEST: [33103 33106 33110 ... 73695 73698 73701]\n",
      "TRAIN: [     0      1      2 ... 135553 135554 135555] TEST: [44339 44342 44344 ... 92005 92008 92010]\n",
      "TRAIN: [     0      1      2 ... 135553 135554 135555] TEST: [55315 55320 55326 ... 97512 97513 97514]\n",
      "TRAIN: [     0      1      2 ... 135553 135554 135555] TEST: [ 66269  66274  66277 ... 102909 102910 102911]\n",
      "TRAIN: [     0      1      2 ... 135553 135554 135555] TEST: [ 77389  77390  77398 ... 124346 124347 124349]\n",
      "TRAIN: [     0      1      2 ... 135553 135554 135555] TEST: [ 88408  88409  88411 ... 130186 130187 130188]\n",
      "TRAIN: [     0      1      2 ... 130186 130187 130188] TEST: [113774 113775 113776 ... 135553 135554 135555]\n"
     ]
    }
   ],
   "source": [
    "tfv = TfidfVectorizer()\n",
    "X2 = tfv. fit_transform(x)\n",
    "#X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y, test_size=0.33, random_state=42)\n",
    "\n",
    "#StratifiedKFold 分层k折\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "skf.get_n_splits(X2, y)\n",
    "for train_index, test_index in skf.split(X2, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X2_train, X2_test = X2[train_index], X2[test_index]\n",
    "    y2_train, y2_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdeefd6",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "After segregating the data, our next work is to find a good algorithm suited for our model. We can use a Decision tree classifier for building the Hate Speech detection project. Decision Trees are a type of Supervised Machine Learning used mainly for classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba6cb9f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:11:21.943326Z",
     "start_time": "2023-02-27T11:09:02.025183Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model building\n",
    "model = DecisionTreeClassifier()\n",
    "#Training the model\n",
    "model. fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a64c943",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:14:06.619908Z",
     "start_time": "2023-02-27T11:11:58.024060Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = DecisionTreeClassifier()\n",
    "#Training the model\n",
    "model2. fit(X2_train,y2_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95785907",
   "metadata": {},
   "source": [
    "## Evaluating the results\n",
    "The final step in machine learning model building is prediction. In this step, we can measure how well our model performs for the test input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5754513d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:14:15.949723Z",
     "start_time": "2023-02-27T11:14:15.916832Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hate speech', 'hate speech', 'hate speech', ..., 'supportive',\n",
       "       'supportive', 'supportive'], dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing the model\n",
    "y_pred = model. predict (X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f627e171",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:14:16.773486Z",
     "start_time": "2023-02-27T11:14:16.733133Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hate speech', 'hate speech', 'hate speech', ..., 'supportive',\n",
       "       'supportive', 'hate speech'], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing the model\n",
    "y2_pred = model2. predict (X2_test)\n",
    "y2_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "044708c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:14:18.919256Z",
     "start_time": "2023-02-27T11:14:18.624217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8393950571744744\n",
      "0.8393950571744744\n",
      "0.8393950571744744\n"
     ]
    }
   ],
   "source": [
    "#Accuracy Score of our model\n",
    "from sklearn. metrics import accuracy_score, f1_score, recall_score\n",
    "print (accuracy_score (y_test,y_pred))\n",
    "print (f1_score (y_test,y_pred, average='micro'))\n",
    "print (recall_score (y_test,y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef3303d",
   "metadata": {},
   "source": [
    "### skmetrics输出acc、precision、recall、f1值相同的问题\n",
    "average='micro'的原理是：\n",
    "把每个类别的TP、FP、FN先相加，再把这个问题当成二分类来进行计算\n",
    "\n",
    "在某一类中被判断成FP的样本，在其他类中一定是FN的样本\n",
    "\n",
    "解决方法的话就是换一种平均的方法average = 'macro’\n",
    "\n",
    "这种方法是对于不同的类分别计算评估指标，然后加起来求平均\n",
    "https://blog.csdn.net/fujikoo/article/details/119926390"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e17b8682",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:14:21.490946Z",
     "start_time": "2023-02-27T11:14:21.225465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8393950571744744\n",
      "0.824345058618583\n",
      "0.8287084916363843\n"
     ]
    }
   ],
   "source": [
    "#Accuracy Score of our model\n",
    "from sklearn. metrics import accuracy_score, f1_score, recall_score\n",
    "print (accuracy_score (y_test,y_pred))\n",
    "print (f1_score (y_test,y_pred, average='macro'))\n",
    "print (recall_score (y_test,y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58935520",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:14:22.712984Z",
     "start_time": "2023-02-27T11:14:22.417451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8906676503135375\n",
      "0.8764654094066571\n",
      "0.8759275550528535\n"
     ]
    }
   ],
   "source": [
    "#Accuracy Score of our model2\n",
    "from sklearn. metrics import accuracy_score\n",
    "print (accuracy_score (y2_test,y2_pred))\n",
    "print (f1_score (y2_test,y2_pred, average='macro'))\n",
    "print (recall_score (y2_test,y2_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "be3af06d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:20:01.549342Z",
     "start_time": "2023-02-27T11:20:01.534918Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['supportive']\n"
     ]
    }
   ],
   "source": [
    "#Predicting the outcome\n",
    "inp = \"fuck russians\"\n",
    "inp = cv.transform([inp]).toarray()\n",
    "print(model.predict(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b7b90373",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:20:08.398960Z",
     "start_time": "2023-02-27T11:20:08.382682Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['supportive']\n"
     ]
    }
   ],
   "source": [
    "#Predicting the outcome\n",
    "inp = \"fuck russians\"\n",
    "inp = tfv.transform([inp]).toarray()\n",
    "print(model2.predict(inp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13463576",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this article, we have built a project for Hate Speech detection using Machine Learning. Hate speech is one of the serious issues we see on social media platforms like Facebook and Twitter. Hope you enjoyed this article by building a project to detect hate speech with Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24feee63",
   "metadata": {},
   "source": [
    "# Use Russia-Ukraine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0dc5e2c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:27:40.389687Z",
     "start_time": "2023-02-27T11:27:40.333110Z"
    }
   },
   "outputs": [],
   "source": [
    "#\"H:\\课程\\毕业论文\\cleaned\\tweet_ids_day_2022-2-22_clean.csv\"\n",
    "df = pd.read_csv(\"H:/课程/毕业论文/cleaned/tweet_ids_day_2022-2-22_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7087dc62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:27:41.540793Z",
     "start_time": "2023-02-27T11:27:41.530555Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df[df.Tweet_isRT==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e923db76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:27:42.856682Z",
     "start_time": "2023-02-27T11:27:42.834731Z"
    }
   },
   "outputs": [],
   "source": [
    "#model1: cv+ decision tree\n",
    "def hate_detection_1(text):\n",
    "    text = clean(text)\n",
    "    inp = cv.transform([text]).toarray()\n",
    "    result = model.predict(inp)\n",
    "    return result[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f16dc6fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:27:43.520018Z",
     "start_time": "2023-02-27T11:27:43.499176Z"
    }
   },
   "outputs": [],
   "source": [
    "#model1: tf-idf+ decision tree\n",
    "def hate_detection_2(text):\n",
    "    text = clean(text)\n",
    "    inp = tfv.transform([text]).toarray()\n",
    "    result = model2.predict(inp)\n",
    "    return result[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7ffcbea1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:27:45.169626Z",
     "start_time": "2023-02-27T11:27:44.589533Z"
    }
   },
   "outputs": [],
   "source": [
    "df['is_Hate_m1'] = df['Tweet_content'].apply(hate_detection_1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9f7bed9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:27:46.710673Z",
     "start_time": "2023-02-27T11:27:45.758268Z"
    }
   },
   "outputs": [],
   "source": [
    "df['is_Hate_m2'] = df['Tweet_content'].apply(hate_detection_2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9c42e456",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:27:47.378450Z",
     "start_time": "2023-02-27T11:27:47.354497Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "supportive     245\n",
       "neutral        115\n",
       "hate speech     26\n",
       "Name: is_Hate_m1, dtype: int64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.is_Hate_m1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c8d85d0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:27:48.175825Z",
     "start_time": "2023-02-27T11:27:48.151291Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "supportive     269\n",
       "neutral         94\n",
       "hate speech     23\n",
       "Name: is_Hate_m2, dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.is_Hate_m2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "733338f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:27:51.484898Z",
     "start_time": "2023-02-27T11:27:51.462479Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'why ukraina is fighting back russia, learn from our modiji simply ban russian apps and declare victory. russiaukrainecrisis russiaukraineconflict ukrainewar'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['is_Hate_m1']==\"hate speech\"].Tweet_content.iloc[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e72e75ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:28:04.753875Z",
     "start_time": "2023-02-27T11:28:04.737010Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'russian map is now bigger than the older one russian ukrainian donbass crimea moscow ukrainerussiacrisis ukraina kiev'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['is_Hate_m2']==\"hate speech\"].Tweet_content.iloc[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0d65a9",
   "metadata": {},
   "source": [
    "# 尝试换一下模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0a49a2b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:28:13.427258Z",
     "start_time": "2023-02-27T11:28:12.814306Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#让我们从朴素的贝叶斯分类器开始，它为该任务提供了一个很好的基准。 scikit-learn包含此分类器的多种变体； 多项式最适合单词计数：\n",
    "clf = MultinomialNB().fit(X2_train,y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b49f0093",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:28:14.269312Z",
     "start_time": "2023-02-27T11:28:14.232158Z"
    }
   },
   "outputs": [],
   "source": [
    "predicted = clf.predict(X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "82048d4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:28:15.793261Z",
     "start_time": "2023-02-27T11:28:15.278556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8425673183327186\n",
      "0.800798623542625\n",
      "0.7944992042351556\n"
     ]
    }
   ],
   "source": [
    "print (accuracy_score (y2_test,predicted))\n",
    "print (f1_score (y2_test,predicted, average='macro'))\n",
    "print (recall_score (y2_test,predicted, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f8aaf8bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:31:59.956147Z",
     "start_time": "2023-02-27T11:31:59.352029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 11128  11129  11130 ... 135553 135554 135555] TEST: [    0     1     2 ... 18499 18501 18503]\n",
      "TRAIN: [     0      1      2 ... 135553 135554 135555] TEST: [11128 11129 11130 ... 37097 37105 37106]\n",
      "TRAIN: [     0      1      2 ... 135553 135554 135555] TEST: [22146 22154 22159 ... 55224 55226 55228]\n",
      "TRAIN: [     0      1      2 ... 135553 135554 135555] TEST: [33103 33106 33110 ... 73695 73698 73701]\n",
      "TRAIN: [     0      1      2 ... 135553 135554 135555] TEST: [44339 44342 44344 ... 92005 92008 92010]\n",
      "TRAIN: [     0      1      2 ... 135553 135554 135555] TEST: [55315 55320 55326 ... 97512 97513 97514]\n",
      "TRAIN: [     0      1      2 ... 135553 135554 135555] TEST: [ 66269  66274  66277 ... 102909 102910 102911]\n",
      "TRAIN: [     0      1      2 ... 135553 135554 135555] TEST: [ 77389  77390  77398 ... 124346 124347 124349]\n",
      "TRAIN: [     0      1      2 ... 135553 135554 135555] TEST: [ 88408  88409  88411 ... 130186 130187 130188]\n",
      "TRAIN: [     0      1      2 ... 130186 130187 130188] TEST: [113774 113775 113776 ... 135553 135554 135555]\n"
     ]
    }
   ],
   "source": [
    "x = np. array(subdata[\"text\"])\n",
    "y = np. array(subdata[\"label\"])\n",
    "# Splitting the Data\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "skf.get_n_splits(x, y)\n",
    "for train_index, test_index in skf.split(x, y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X3_train, X3_test = x[train_index], x[test_index]\n",
    "    y3_train, y3_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ce01d89d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:32:02.902510Z",
     "start_time": "2023-02-27T11:32:02.883346Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9a07d232",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:32:16.463120Z",
     "start_time": "2023-02-27T11:32:10.237736Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, CountVectorizer()), (&#x27;tfidf&#x27;, TfidfTransformer()),\n",
       "                (&#x27;clf&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, CountVectorizer()), (&#x27;tfidf&#x27;, TfidfTransformer()),\n",
       "                (&#x27;clf&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
       "                ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(X3_train,y3_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "10457cd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-27T11:32:41.522908Z",
     "start_time": "2023-02-27T11:32:35.097007Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7576540022132054\n",
      "0.588504571982918\n",
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "#线性支持向量机（SVM）\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                          alpha=1e-3, random_state=42,\n",
    "                          max_iter=5, tol=None)),\n",
    "])\n",
    "\n",
    "text_clf.fit(X3_train, y3_train)\n",
    "y3_pred = text_clf.predict(X3_test)\n",
    "print (accuracy_score (y3_test,y3_pred))\n",
    "print (f1_score (y3_test,y3_pred, average='macro'))\n",
    "print (recall_score (y3_test,y3_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeafb294",
   "metadata": {},
   "source": [
    "换了以后还不如之前的呢……"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4837531",
   "metadata": {},
   "source": [
    "# 优化工作\n",
    "从上面的结果可以看出，目前的分类器虽然在测试集上表现不错，但泛化性能很差。可能可以从以下方向尝试排查问题并优化分类器：\n",
    "1. 考虑过拟合，学习相关代码\n",
    "2. 检查训练集的数据格式与俄乌战争推特有无区别，将俄乌战争推特进一步清洗或转换成训练集一致格式\n",
    "3. 优化特征工程，如加入句法标注器nltk pos tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55de3f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
