{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54f5e557",
   "metadata": {},
   "source": [
    "# 使用Snscrape+多线程爬取推文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3d05a18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T01:03:39.537952Z",
     "start_time": "2023-02-16T01:03:35.965196Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import sys\n",
    "import queue\n",
    "import threading\n",
    "from IPython.display import clear_output as clear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca2cfa0",
   "metadata": {},
   "source": [
    "Tweet_id存储在年月格式的文件夹中，名称有固定格式，按照解析式列出文件名并选出实际存在的文件名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29398030",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T01:03:45.066201Z",
     "start_time": "2023-02-16T01:03:40.441477Z"
    }
   },
   "outputs": [],
   "source": [
    "date_series = [f\"D:\\\\hrfan\\\\RussoUkrainianWar_Dataset-main\\\\2022-0{month}\\\\tweet_ids_day_2022-{month}-{date}.txt\"\n",
    "              for month in range(2,10) for date in range(1,32)]\n",
    "dates = []\n",
    "for date_data in date_series:\n",
    "    if os.path.exists(date_data):\n",
    "        with open(date_data,'r') as f:\n",
    "            dates.append(date_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96a825cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T01:03:45.769795Z",
     "start_time": "2023-02-16T01:03:45.760787Z"
    }
   },
   "outputs": [],
   "source": [
    "dates = dates[20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe62e387",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-16T01:03:47.220407Z",
     "start_time": "2023-02-16T01:03:47.203399Z"
    }
   },
   "outputs": [],
   "source": [
    "sem=threading.BoundedSemaphore(42) #限制线程的最大数量42，可以根据电脑性能调整线程数\n",
    "def tweetscraper(j, tweets_list):\n",
    "    with sem:\n",
    "        clear(wait=True)#先清除输出区域，不然会一直打印造成ipynb文件过大\n",
    "        try:\n",
    "            for i,tweet in enumerate(sntwitter.TwitterTweetScraper(j).get_items()):\n",
    "                tweets_list.append([\n",
    "                    #这里有些用的是老版本的变量名，会提示使用新的名称，但由于我最开始就用老名称爬就没改\n",
    "                    tweet.date, tweet.url, tweet.id, tweet.content, tweet.replyCount, tweet.retweetCount, tweet.likeCount,\n",
    "                    tweet.quoteCount, tweet.conversationId, tweet.lang, tweet.source, tweet.sourceUrl, tweet.outlinks, tweet.media,\n",
    "                    tweet.retweetedTweet, tweet.quotedTweet, tweet.inReplyToTweetId, tweet.inReplyToUser, tweet.mentionedUsers, \n",
    "                    tweet.coordinates, tweet.place, tweet.hashtags, tweet.cashtags, #tweet相关的字段\n",
    "                    tweet.user.username, tweet.user.displayname, tweet.user.id, tweet.user.rawDescription, tweet.user.descriptionUrls,\n",
    "                    tweet.user.verified, tweet.user.created,tweet.user.followersCount, tweet.user.friendsCount, tweet.user.statusesCount, \n",
    "                    tweet.user.favouritesCount,tweet.user.listedCount, tweet.user.mediaCount, tweet.user.location, tweet.user.protected,\n",
    "                    tweet.user.linkUrl,tweet.user.profileImageUrl, tweet.user.profileBannerUrl, tweet.user.label ##user相关的字段\n",
    "                    ])\n",
    "            time.sleep(1)\n",
    "            #print(len(tweets_list), \"tweets have been scraped\", flush = True)\n",
    "        except:\n",
    "            print(\"a mistake have been passed\", flush = True)\n",
    "            time.sleep(3)#多休息一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67680173",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-02-16T01:03:56.661Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://api.twitter.com/1.1/guest/activate.json: non-200 response (429)\n",
      "4 requests to https://api.twitter.com/1.1/guest/activate.json failed, giving up.\n",
      "Errors: non-200 response (429), non-200 response (429), non-200 response (429), non-200 response (429)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a mistake have been passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://api.twitter.com/1.1/guest/activate.json: non-200 response (429)\n",
      "4 requests to https://api.twitter.com/1.1/guest/activate.json failed, giving up.\n",
      "Errors: non-200 response (429), non-200 response (429), non-200 response (429), non-200 response (429)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a mistake have been passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://api.twitter.com/1.1/guest/activate.json: non-200 response (429)\n",
      "4 requests to https://api.twitter.com/1.1/guest/activate.json failed, giving up.\n",
      "Errors: non-200 response (429), non-200 response (429), non-200 response (429), non-200 response (429)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a mistake have been passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://api.twitter.com/1.1/guest/activate.json: non-200 response (429)\n",
      "4 requests to https://api.twitter.com/1.1/guest/activate.json failed, giving up.\n",
      "Errors: non-200 response (429), non-200 response (429), non-200 response (429), non-200 response (429)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a mistake have been passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://api.twitter.com/1.1/guest/activate.json: non-200 response (429)\n",
      "4 requests to https://api.twitter.com/1.1/guest/activate.json failed, giving up.\n",
      "Errors: non-200 response (429), non-200 response (429), non-200 response (429), non-200 response (429)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a mistake have been passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://api.twitter.com/1.1/guest/activate.json: non-200 response (429)\n",
      "4 requests to https://api.twitter.com/1.1/guest/activate.json failed, giving up.\n",
      "Errors: non-200 response (429), non-200 response (429), non-200 response (429), non-200 response (429)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a mistake have been passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://api.twitter.com/1.1/guest/activate.json: non-200 response (429)\n",
      "4 requests to https://api.twitter.com/1.1/guest/activate.json failed, giving up.\n",
      "Errors: non-200 response (429), non-200 response (429), non-200 response (429), non-200 response (429)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a mistake have been passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://api.twitter.com/1.1/guest/activate.json: non-200 response (429)\n",
      "4 requests to https://api.twitter.com/1.1/guest/activate.json failed, giving up.\n",
      "Errors: non-200 response (429), non-200 response (429), non-200 response (429), non-200 response (429)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a mistake have been passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://api.twitter.com/1.1/guest/activate.json: non-200 response (429)\n",
      "4 requests to https://api.twitter.com/1.1/guest/activate.json failed, giving up.\n",
      "Errors: non-200 response (429), non-200 response (429), non-200 response (429), non-200 response (429)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a mistake have been passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://api.twitter.com/1.1/guest/activate.json: non-200 response (429)\n",
      "4 requests to https://api.twitter.com/1.1/guest/activate.json failed, giving up.\n",
      "Errors: non-200 response (429), non-200 response (429), non-200 response (429), non-200 response (429)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a mistake have been passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://api.twitter.com/1.1/guest/activate.json: non-200 response (429)\n",
      "4 requests to https://api.twitter.com/1.1/guest/activate.json failed, giving up.\n",
      "Errors: non-200 response (429), non-200 response (429), non-200 response (429), non-200 response (429)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a mistake have been passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://api.twitter.com/1.1/guest/activate.json: non-200 response (429)\n",
      "4 requests to https://api.twitter.com/1.1/guest/activate.json failed, giving up.\n",
      "Errors: non-200 response (429), non-200 response (429), non-200 response (429), non-200 response (429)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a mistake have been passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error retrieving https://api.twitter.com/1.1/guest/activate.json: non-200 response (429)\n",
      "4 requests to https://api.twitter.com/1.1/guest/activate.json failed, giving up.\n",
      "Errors: non-200 response (429), non-200 response (429), non-200 response (429), non-200 response (429)\n"
     ]
    }
   ],
   "source": [
    "#开始爬！\n",
    "#ver1.0：https://blog.wangtwothree.com/code/196.html 多线程的内存占用问题\n",
    "#ver2.0：还是出现崩溃现象，https://stackoverflow.com/questions/69005764/iostream-flush-timed-out-errors-when-multithreading\n",
    "#找到了和自己情况一样的提问，但没有找到解决方案。重写代码以实现把一个文件分成好几个来保存，看能不能解决这个问题\n",
    "\n",
    "for i in range(len(dates)):\n",
    "    rootname = dates[i] +'_tweets.csv'#之前输出的文件地址\n",
    "    nametime = 1\n",
    "    name = dates[i]+'_tweets'+'_'+str(nametime)+'.csv'\n",
    "    if os.path.exists(rootname):#如果已经爬过了就跳过\n",
    "        print(rootname, ' has been scraped, passed.', flush=True)\n",
    "        continue\n",
    "        \n",
    "    \n",
    "        \n",
    "    else:\n",
    "        while os.path.exists(name):\n",
    "            nametime+=1\n",
    "            name = dates[i]+'_tweets'+'_'+str(nametime)+'.csv'\n",
    "            \n",
    "        testset = []\n",
    "        with open(dates[i], 'r') as f:\n",
    "            for line in f:\n",
    "                testset.append(line.strip('\\\\n').split(','))\n",
    "        test = []\n",
    "        \n",
    "        for j in testset:\n",
    "            test.append(int(j[0]))\n",
    "            \n",
    "        if nametime>1:\n",
    "            test = test[500000*(nametime-1):]#如果之前已经爬了就跳过\n",
    "        # create multi-threads to scrape\n",
    "        threads = []\n",
    "        tweets_list = []\n",
    "        print(dates[i], 'scraper begins')\n",
    "\n",
    "        \n",
    "        for j in test:\n",
    "            if (test.index(j)+1)%500000!=0:#每50万条保存一个子csv文件\n",
    "                t = threading.Thread(target = tweetscraper, args=(j, tweets_list))\n",
    "                t.start()\n",
    "                threads.append(t)\n",
    "            else:\n",
    "                for t in threads:\n",
    "                    t.join()\n",
    "                    threads.remove(t)#为了内存稳定，需要remove之前的线程，否则数据过多的时候会卡死，功亏一篑。加入这行代码后内存占用有改善但还是没用\n",
    "                name = dates[i]+'_tweets'+'_'+str(nametime)+'.csv'\n",
    "                nametime+=1\n",
    "                tweets_df = pd.DataFrame(tweets_list, columns=[\n",
    "                                                    'Datetime', 'Tweet_url', 'Tweet_id', 'Tweet_content', 'Tweet_reply_count', 'Tweet_retweet_count',\n",
    "                                                    'Tweet_like_count', 'Tweet_quote_count', 'Tweet_conversation_id', 'Tweet_language', 'Tweet_source', \n",
    "                                                    'Tweet_source_url', 'Tweet_links', 'Tweet_mdeia', 'Tweet_retweeted_tweet', 'Tweet_quoted_tweet', \n",
    "                                                    'Tweet_inReplyToTweetId','Tweet_inReplyToUser','Tweet_mentioned_users','Tweet_coordinates', 'Tweet_place', \n",
    "                                                    'Tweet_hashtags', 'Tweet_cashtags',#tweet 相关的字段\n",
    "                                                    'Username','User_displayname', 'User_id', 'User_profile_description', 'User_description_link', 'User_verified',\n",
    "                                                    'User_created', 'User_followers_count', 'User_friends_count', 'User_statuses_count', 'User_favourites_count',\n",
    "                                                    'User_listed_count', 'User_media_count', 'User_location', 'User_protected', 'User_profile_link', 'User_profile_image_url',\n",
    "                                                    'User_profile_banner_url', 'User_label' #user 相关的字段\n",
    "                                                  ])\n",
    "\n",
    "                tweets_df.to_csv(name)\n",
    "                tweets_list=[]\n",
    "            \n",
    "        print(dates[i], 'has been scraped')\n",
    "        for t in threads:\n",
    "            t.join()\n",
    "            threads.remove(t)#为了内存稳定，需要remove之前的线程，否则数据过多的时候会卡死，功亏一篑。加入这行代码后内存占用有改善但还是没用\n",
    "        \n",
    "        name = dates[i]+'_tweets'+'_'+str(nametime)+'.csv'\n",
    "        tweets_df = pd.DataFrame(tweets_list, columns=[\n",
    "                                                    'Datetime', 'Tweet_url', 'Tweet_id', 'Tweet_content', 'Tweet_reply_count', 'Tweet_retweet_count',\n",
    "                                                    'Tweet_like_count', 'Tweet_quote_count', 'Tweet_conversation_id', 'Tweet_language', 'Tweet_source', \n",
    "                                                    'Tweet_source_url', 'Tweet_links', 'Tweet_mdeia', 'Tweet_retweeted_tweet', 'Tweet_quoted_tweet', \n",
    "                                                    'Tweet_inReplyToTweetId','Tweet_inReplyToUser','Tweet_mentioned_users','Tweet_coordinates', 'Tweet_place', \n",
    "                                                    'Tweet_hashtags', 'Tweet_cashtags',#tweet 相关的字段\n",
    "                                                    'Username','User_displayname', 'User_id', 'User_profile_description', 'User_description_link', 'User_verified',\n",
    "                                                    'User_created', 'User_followers_count', 'User_friends_count', 'User_statuses_count', 'User_favourites_count',\n",
    "                                                    'User_listed_count', 'User_media_count', 'User_location', 'User_protected', 'User_profile_link', 'User_profile_image_url',\n",
    "                                                    'User_profile_banner_url', 'User_label' #user 相关的字段\n",
    "                                                  ])\n",
    "\n",
    "        tweets_df.to_csv(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 153.844,
   "position": {
    "height": "215.844px",
    "left": "1108px",
    "right": "20px",
    "top": "164px",
    "width": "294px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
